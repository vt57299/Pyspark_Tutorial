{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DDIzyRgkJSyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5bfa9d-067d-4ad4-fb05-8ae1e8f44b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "7acWLYm6J6ZJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Always start pyspark session before working with spark\n",
        "spark = SparkSession.builder.appName('Practice').getOrCreate()"
      ],
      "metadata": {
        "id": "sdNOpdO5JVeP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "cRo_tsydJ2Lj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "f47d1dc1-5169-4759-dc96-8e7c9882ead6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7a7fd0244d90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5bd8404c5b96:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Practice</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dRv-Z_PH2cD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5091e6-159e-4e73-c9fe-1ac5b3ef35c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/sample_dataset/Orders.csv'\n",
        "df_pyspark = spark.read.csv(file_path)"
      ],
      "metadata": {
        "id": "6BOiripeKBaK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "p4phzE1d3bq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0aeeaa9-4499-49f9-886c-fbbc50221eed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+----------+------------+\n",
            "|     _c0|        _c1|         _c2|       _c3|         _c4|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "|order_id|customer_id|warehouse_id|order_date|shipper_date|\n",
            "|     789|       3731|        8118|01/01/2019|  01/04/2019|\n",
            "|     790|       3486|        8118|  1/1/2019|    1/4/2019|\n",
            "|     791|       2623|        8118|  1/1/2019|    1/4/2019|\n",
            "|     792|       9869|        8118|  1/1/2019|    1/4/2019|\n",
            "|     793|       6866|        8118|  1/1/2019|    1/4/2019|\n",
            "|     794|       8055|        8118|  1/1/2019|    1/4/2019|\n",
            "|     795|       1152|        8118|  1/1/2019|    1/4/2019|\n",
            "|     796|       5765|        8118|  1/1/2019|    1/4/2019|\n",
            "|     797|       6709|        8118|  1/1/2019|    1/4/2019|\n",
            "|     798|       4866|        2666|  1/1/2019|    1/4/2019|\n",
            "|     799|       4515|        2666|  1/1/2019|    1/4/2019|\n",
            "|     800|       9618|        2666|  1/1/2019|    1/4/2019|\n",
            "|     801|       2337|        2666|  1/1/2019|    1/4/2019|\n",
            "|     802|       1166|        2666|  1/1/2019|    1/4/2019|\n",
            "|     803|       4376|        2666|  1/1/2019|    1/4/2019|\n",
            "|     804|       9832|        2666|  1/1/2019|    1/4/2019|\n",
            "|     805|       6046|        9080|  1/1/2019|    1/4/2019|\n",
            "|     806|       6046|        1543|  1/1/2019|    1/4/2019|\n",
            "|     807|       3710|        6509|  1/1/2019|    1/4/2019|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To fix the headers\n",
        "df_pyspark = spark.read.option('header', 'true').csv(file_path)"
      ],
      "metadata": {
        "id": "fXog3g5nKjDY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "Q_V6xnUrKkbp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd526d32-92d1-4529-f224-17216d8b22df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+----------+------------+\n",
            "|order_id|customer_id|warehouse_id|order_date|shipper_date|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "|     789|       3731|        8118|01/01/2019|  01/04/2019|\n",
            "|     790|       3486|        8118|  1/1/2019|    1/4/2019|\n",
            "|     791|       2623|        8118|  1/1/2019|    1/4/2019|\n",
            "|     792|       9869|        8118|  1/1/2019|    1/4/2019|\n",
            "|     793|       6866|        8118|  1/1/2019|    1/4/2019|\n",
            "|     794|       8055|        8118|  1/1/2019|    1/4/2019|\n",
            "|     795|       1152|        8118|  1/1/2019|    1/4/2019|\n",
            "|     796|       5765|        8118|  1/1/2019|    1/4/2019|\n",
            "|     797|       6709|        8118|  1/1/2019|    1/4/2019|\n",
            "|     798|       4866|        2666|  1/1/2019|    1/4/2019|\n",
            "|     799|       4515|        2666|  1/1/2019|    1/4/2019|\n",
            "|     800|       9618|        2666|  1/1/2019|    1/4/2019|\n",
            "|     801|       2337|        2666|  1/1/2019|    1/4/2019|\n",
            "|     802|       1166|        2666|  1/1/2019|    1/4/2019|\n",
            "|     803|       4376|        2666|  1/1/2019|    1/4/2019|\n",
            "|     804|       9832|        2666|  1/1/2019|    1/4/2019|\n",
            "|     805|       6046|        9080|  1/1/2019|    1/4/2019|\n",
            "|     806|       6046|        1543|  1/1/2019|    1/4/2019|\n",
            "|     807|       3710|        6509|  1/1/2019|    1/4/2019|\n",
            "|     808|       7025|        6509|  1/1/2019|    1/4/2019|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df_pyspark)"
      ],
      "metadata": {
        "id": "TkPnkuLKLVVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "30ad543b-2331-4c63-cd8c-36c11399b595"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.head(3)"
      ],
      "metadata": {
        "id": "3tzziAuaLYeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab89e186-2052-46fa-dbb8-1e0eedd0204e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(order_id='789', customer_id='3731', warehouse_id='8118', order_date='01/01/2019', shipper_date='01/04/2019'),\n",
              " Row(order_id='790', customer_id='3486', warehouse_id='8118', order_date='1/1/2019', shipper_date='1/4/2019'),\n",
              " Row(order_id='791', customer_id='2623', warehouse_id='8118', order_date='1/1/2019', shipper_date='1/4/2019')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# More information regarding my columns\n",
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "LSHqfGAtLf8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6050d39f-8954-4b08-fe05-e32ac1c9ce07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- warehouse_id: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- shipper_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Pyspark DataFrames- Part 1"
      ],
      "metadata": {
        "id": "BgLTvxIxMHS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  read the dataset\n",
        "# inferSchema - If not used it all columns will be of string datatype.\n",
        "df_pyspark = spark.read.option('header','true').csv(file_path, inferSchema = True)"
      ],
      "metadata": {
        "id": "8lndc-yiMHPe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the datatypes of the column (Schema)\n",
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "40AYrEecMHNI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561a2991-5708-4532-88e6-fa79dae653b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- warehouse_id: integer (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- shipper_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best way to read csv file in pyspark\n",
        "df_pyspark = spark.read.csv(file_path, header = True, inferSchema = True)\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "BqiSTBQJMHKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe6064f-ec02-4287-e34b-e75cd42ea016"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+----------+------------+\n",
            "|order_id|customer_id|warehouse_id|order_date|shipper_date|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "|     789|       3731|        8118|01/01/2019|  01/04/2019|\n",
            "|     790|       3486|        8118|  1/1/2019|    1/4/2019|\n",
            "|     791|       2623|        8118|  1/1/2019|    1/4/2019|\n",
            "|     792|       9869|        8118|  1/1/2019|    1/4/2019|\n",
            "|     793|       6866|        8118|  1/1/2019|    1/4/2019|\n",
            "|     794|       8055|        8118|  1/1/2019|    1/4/2019|\n",
            "|     795|       1152|        8118|  1/1/2019|    1/4/2019|\n",
            "|     796|       5765|        8118|  1/1/2019|    1/4/2019|\n",
            "|     797|       6709|        8118|  1/1/2019|    1/4/2019|\n",
            "|     798|       4866|        2666|  1/1/2019|    1/4/2019|\n",
            "|     799|       4515|        2666|  1/1/2019|    1/4/2019|\n",
            "|     800|       9618|        2666|  1/1/2019|    1/4/2019|\n",
            "|     801|       2337|        2666|  1/1/2019|    1/4/2019|\n",
            "|     802|       1166|        2666|  1/1/2019|    1/4/2019|\n",
            "|     803|       4376|        2666|  1/1/2019|    1/4/2019|\n",
            "|     804|       9832|        2666|  1/1/2019|    1/4/2019|\n",
            "|     805|       6046|        9080|  1/1/2019|    1/4/2019|\n",
            "|     806|       6046|        1543|  1/1/2019|    1/4/2019|\n",
            "|     807|       3710|        6509|  1/1/2019|    1/4/2019|\n",
            "|     808|       7025|        6509|  1/1/2019|    1/4/2019|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the datatype of the dataframe\n",
        "type(df_pyspark)"
      ],
      "metadata": {
        "id": "KLyrbvI9MHH5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "76c5d051-8ebe-44e5-b739-e5a6e9f34969"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataframe - Dataframe is data structure, because inside this we can perform various kinds of operations."
      ],
      "metadata": {
        "id": "WFlhN-7fMHF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting columns and Indexing"
      ],
      "metadata": {
        "id": "0ElF8DQaMHDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking column names\n",
        "df_pyspark.columns"
      ],
      "metadata": {
        "id": "odIcyqWvMHAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa72b14-5113-45af-c4d1-bc422a811386"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['order_id', 'customer_id', 'warehouse_id', 'order_date', 'shipper_date']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking first few columns\n",
        "df_pyspark.head(3)"
      ],
      "metadata": {
        "id": "dHtEBoEvMG9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab77ef2-f5f4-4b68-9a62-e29e858cc9f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(order_id=789, customer_id=3731, warehouse_id=8118, order_date='01/01/2019', shipper_date='01/04/2019'),\n",
              " Row(order_id=790, customer_id=3486, warehouse_id=8118, order_date='1/1/2019', shipper_date='1/4/2019'),\n",
              " Row(order_id=791, customer_id=2623, warehouse_id=8118, order_date='1/1/2019', shipper_date='1/4/2019')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select one specific column\n",
        "df_pyspark.select('order_id').show()  # for 1 column\n",
        "# type(df_pyspark.select('order_id'))   # datatype of 1 column\n"
      ],
      "metadata": {
        "id": "kNZd-cN0MG7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e233db-40b8-49c0-8b6d-31ba61d3ed35"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|order_id|\n",
            "+--------+\n",
            "|     789|\n",
            "|     790|\n",
            "|     791|\n",
            "|     792|\n",
            "|     793|\n",
            "|     794|\n",
            "|     795|\n",
            "|     796|\n",
            "|     797|\n",
            "|     798|\n",
            "|     799|\n",
            "|     800|\n",
            "|     801|\n",
            "|     802|\n",
            "|     803|\n",
            "|     804|\n",
            "|     805|\n",
            "|     806|\n",
            "|     807|\n",
            "|     808|\n",
            "+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to pickup multiple columns\n",
        "df_pyspark.select(['order_id','customer_id']).show()"
      ],
      "metadata": {
        "id": "0a4xB61HMG4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236a5ff5-16d9-4c85-d383-be9d71b985b8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+\n",
            "|order_id|customer_id|\n",
            "+--------+-----------+\n",
            "|     789|       3731|\n",
            "|     790|       3486|\n",
            "|     791|       2623|\n",
            "|     792|       9869|\n",
            "|     793|       6866|\n",
            "|     794|       8055|\n",
            "|     795|       1152|\n",
            "|     796|       5765|\n",
            "|     797|       6709|\n",
            "|     798|       4866|\n",
            "|     799|       4515|\n",
            "|     800|       9618|\n",
            "|     801|       2337|\n",
            "|     802|       1166|\n",
            "|     803|       4376|\n",
            "|     804|       9832|\n",
            "|     805|       6046|\n",
            "|     806|       6046|\n",
            "|     807|       3710|\n",
            "|     808|       7025|\n",
            "+--------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_pyspark['order_id'].show()\n",
        "# this wont work. we need to use select function"
      ],
      "metadata": {
        "id": "5jR26MukMG2A"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datatypes of columns\n",
        "df_pyspark.dtypes"
      ],
      "metadata": {
        "id": "SRV3PApoMGzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3801f99c-3492-4e04-ca88-f0a1e49603e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('order_id', 'int'),\n",
              " ('customer_id', 'int'),\n",
              " ('warehouse_id', 'int'),\n",
              " ('order_date', 'string'),\n",
              " ('shipper_date', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Describe option: similar to pandas\n",
        "df_pyspark.describe().show()"
      ],
      "metadata": {
        "id": "4D5NxKiJMGwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6734fa05-b2a3-4dac-b0a6-8a12a28ce9dd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+-----------------+------------------+----------+------------+\n",
            "|summary|         order_id|      customer_id|      warehouse_id|order_date|shipper_date|\n",
            "+-------+-----------------+-----------------+------------------+----------+------------+\n",
            "|  count|             9999|             9999|              9999|      9999|        9999|\n",
            "|   mean|           5788.0|5470.125812581258| 5556.591559155916|      NULL|        NULL|\n",
            "| stddev|2886.607004772212|2601.663953249516|2532.7890496578652|      NULL|        NULL|\n",
            "|    min|              789|             1001|              1543|01/01/2019|  01/01/2020|\n",
            "|    max|            10787|             9999|              9080|  9/9/2019|    9/9/2019|\n",
            "+-------+-----------------+-----------------+------------------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding columns in pyspark dataframe\n",
        "# .show() - is just to show the df\n",
        "df_pyspark.withColumn('new_order_id', df_pyspark['order_id']+1).show(10)"
      ],
      "metadata": {
        "id": "4jTX20U5MGuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b5309a-6b44-43b7-b9f5-9bcf935236ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+----------+------------+------------+\n",
            "|order_id|customer_id|warehouse_id|order_date|shipper_date|new_order_id|\n",
            "+--------+-----------+------------+----------+------------+------------+\n",
            "|     789|       3731|        8118|01/01/2019|  01/04/2019|         790|\n",
            "|     790|       3486|        8118|  1/1/2019|    1/4/2019|         791|\n",
            "|     791|       2623|        8118|  1/1/2019|    1/4/2019|         792|\n",
            "|     792|       9869|        8118|  1/1/2019|    1/4/2019|         793|\n",
            "|     793|       6866|        8118|  1/1/2019|    1/4/2019|         794|\n",
            "|     794|       8055|        8118|  1/1/2019|    1/4/2019|         795|\n",
            "|     795|       1152|        8118|  1/1/2019|    1/4/2019|         796|\n",
            "|     796|       5765|        8118|  1/1/2019|    1/4/2019|         797|\n",
            "|     797|       6709|        8118|  1/1/2019|    1/4/2019|         798|\n",
            "|     798|       4866|        2666|  1/1/2019|    1/4/2019|         799|\n",
            "+--------+-----------+------------+----------+------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To drop the columns\n",
        "df_pyspark.drop('new_order_id').show()"
      ],
      "metadata": {
        "id": "JbQ4K6-hMGrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd7ac38-c0e5-4b01-af9e-320c212d7cf1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------------+----------+------------+\n",
            "|order_id|customer_id|warehouse_id|order_date|shipper_date|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "|     789|       3731|        8118|01/01/2019|  01/04/2019|\n",
            "|     790|       3486|        8118|  1/1/2019|    1/4/2019|\n",
            "|     791|       2623|        8118|  1/1/2019|    1/4/2019|\n",
            "|     792|       9869|        8118|  1/1/2019|    1/4/2019|\n",
            "|     793|       6866|        8118|  1/1/2019|    1/4/2019|\n",
            "|     794|       8055|        8118|  1/1/2019|    1/4/2019|\n",
            "|     795|       1152|        8118|  1/1/2019|    1/4/2019|\n",
            "|     796|       5765|        8118|  1/1/2019|    1/4/2019|\n",
            "|     797|       6709|        8118|  1/1/2019|    1/4/2019|\n",
            "|     798|       4866|        2666|  1/1/2019|    1/4/2019|\n",
            "|     799|       4515|        2666|  1/1/2019|    1/4/2019|\n",
            "|     800|       9618|        2666|  1/1/2019|    1/4/2019|\n",
            "|     801|       2337|        2666|  1/1/2019|    1/4/2019|\n",
            "|     802|       1166|        2666|  1/1/2019|    1/4/2019|\n",
            "|     803|       4376|        2666|  1/1/2019|    1/4/2019|\n",
            "|     804|       9832|        2666|  1/1/2019|    1/4/2019|\n",
            "|     805|       6046|        9080|  1/1/2019|    1/4/2019|\n",
            "|     806|       6046|        1543|  1/1/2019|    1/4/2019|\n",
            "|     807|       3710|        6509|  1/1/2019|    1/4/2019|\n",
            "|     808|       7025|        6509|  1/1/2019|    1/4/2019|\n",
            "+--------+-----------+------------+----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming columns\n",
        "df_pyspark.withColumnRenamed('order_id','new_order_id').show(1)"
      ],
      "metadata": {
        "id": "Vwx5CcI2MGo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e5d51c-b361-4da9-d840-a7690534b878"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+----------+------------+\n",
            "|new_order_id|customer_id|warehouse_id|order_date|shipper_date|\n",
            "+------------+-----------+------------+----------+------------+\n",
            "|         789|       3731|        8118|01/01/2019|  01/04/2019|\n",
            "+------------+-----------+------------+----------+------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Pyspark Handling Missing Values\n",
        "- Dropping Columns\n",
        "- Dropping Rows\n",
        "- Various Parameter in Dropping functionalities\n",
        "- Handling Missing values by Mean, Median and Mode"
      ],
      "metadata": {
        "id": "ji0cZIA5MGmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.appname('Practice').getOrCreate()"
      ],
      "metadata": {
        "id": "MtDIZ9nIMGj6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/sample_dataset/test2.csv'\n",
        "df_pyspark = spark.read.csv(file_path, header = True, inferSchema = True)\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "3k4_XYz9MGhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da1d78c-c14d-47fc-dc5d-ea2ffa788206"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|NULL|      NULL| 40000|\n",
            "|     NULL|  34|        10| 38000|\n",
            "|     NULL|  36|      NULL|  NULL|\n",
            "+---------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the column\n",
        "df_pyspark.drop('Name').show(3)"
      ],
      "metadata": {
        "id": "qN68LYKsMGel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7f7c3b-e375-4eb3-9e81-6855dbee5ed4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+------+\n",
            "|age|Experience|Salary|\n",
            "+---+----------+------+\n",
            "| 31|        10| 30000|\n",
            "| 30|         8| 25000|\n",
            "| 29|         4| 20000|\n",
            "+---+----------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping the specific rows"
      ],
      "metadata": {
        "id": "zHXKZrifMGcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Here: how = 'any' . which drop a row if it contains any nulls. Default\n",
        "df_pyspark.na.drop().show()"
      ],
      "metadata": {
        "id": "dl4EPcRFMGZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d14a9dd-452d-4070-da3a-8038c75adc55"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. how = all. This will drop the row only if the row is completely null.\n",
        "df_pyspark.na.drop(how = 'all').show()"
      ],
      "metadata": {
        "id": "N0K-bSaSMGWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91441804-c0bc-4bf2-9387-356b79dade15"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|NULL|      NULL| 40000|\n",
            "|     NULL|  34|        10| 38000|\n",
            "|     NULL|  36|      NULL|  NULL|\n",
            "+---------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. thresh = threshold value.\n",
        "# Here we set a threshold value = 2, and it will delete the row only if there are atleast 2 null values in n a row\n",
        "df_pyspark.na.drop(how = 'any', thresh = 2).show()"
      ],
      "metadata": {
        "id": "-PCkV3rIL4Iz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379813ac-ad04-450d-fa7d-5d2276bccf19"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|NULL|      NULL| 40000|\n",
            "|     NULL|  34|        10| 38000|\n",
            "+---------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. subset: Just like in Pandas we pass the column name and if that column contains null values then those records will be deleted\n",
        "df_pyspark.na.drop(how = 'any', subset = ['Experience']).show()"
      ],
      "metadata": {
        "id": "JCHl9KhkL6Ow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694186ff-075d-4084-92b7-2242509ef31a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "|     NULL| 34|        10| 38000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill the Missing values\n",
        "df_pyspark.na.fill('Missing Values').show() # This will fill all the null values with Missing Values"
      ],
      "metadata": {
        "id": "qVAJyLgK-psJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5912d80-fd5d-4a36-9dd6-3b319f1ac06b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----+----------+------+\n",
            "|          Name| age|Experience|Salary|\n",
            "+--------------+----+----------+------+\n",
            "|         Krish|  31|        10| 30000|\n",
            "|     Sudhanshu|  30|         8| 25000|\n",
            "|         Sunny|  29|         4| 20000|\n",
            "|          Paul|  24|         3| 20000|\n",
            "|        Harsha|  21|         1| 15000|\n",
            "|       Shubham|  23|         2| 18000|\n",
            "|        Mahesh|NULL|      NULL| 40000|\n",
            "|Missing Values|  34|        10| 38000|\n",
            "|Missing Values|  36|      NULL|  NULL|\n",
            "+--------------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When we want to fill missing values in specific column, we pass the column\n",
        "df_pyspark.na.fill('Missing Values', 'Experience').show()"
      ],
      "metadata": {
        "id": "3T0gOsuN-pop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22de0198-6552-4098-9cd7-da0dd24b5af0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+\n",
            "|     Name| age|Experience|Salary|\n",
            "+---------+----+----------+------+\n",
            "|    Krish|  31|        10| 30000|\n",
            "|Sudhanshu|  30|         8| 25000|\n",
            "|    Sunny|  29|         4| 20000|\n",
            "|     Paul|  24|         3| 20000|\n",
            "|   Harsha|  21|         1| 15000|\n",
            "|  Shubham|  23|         2| 18000|\n",
            "|   Mahesh|NULL|      NULL| 40000|\n",
            "|     NULL|  34|        10| 38000|\n",
            "|     NULL|  36|      NULL|  NULL|\n",
            "+---------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also give multiple columns in a list\n",
        "df_pyspark.na.fill('Missing Values', ['Name', 'age']).show()"
      ],
      "metadata": {
        "id": "D5BX5Mcp-pma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2c97a4-818c-4a19-dbf1-8886a7b6cf1d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----+----------+------+\n",
            "|          Name| age|Experience|Salary|\n",
            "+--------------+----+----------+------+\n",
            "|         Krish|  31|        10| 30000|\n",
            "|     Sudhanshu|  30|         8| 25000|\n",
            "|         Sunny|  29|         4| 20000|\n",
            "|          Paul|  24|         3| 20000|\n",
            "|        Harsha|  21|         1| 15000|\n",
            "|       Shubham|  23|         2| 18000|\n",
            "|        Mahesh|NULL|      NULL| 40000|\n",
            "|Missing Values|  34|        10| 38000|\n",
            "|Missing Values|  36|      NULL|  NULL|\n",
            "+--------------+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling null values with mean, median and mode.\n",
        "from pyspark.ml.feature import Imputer"
      ],
      "metadata": {
        "id": "0FUmjCfp-pjk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = Imputer(inputCols= ['age', 'Experience', 'Salary'],\n",
        "                  outputCols = ['{}_imputed'.format(c) for c in ['age','Experience', 'Salary']]\n",
        "                  ).setStrategy('mode')\n",
        "# we just have to change setStrategy value to mean, median and mode"
      ],
      "metadata": {
        "id": "oXdKBR5s-pgX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add imputation cols to df\n",
        "imputer.fit(df_pyspark).transform(df_pyspark).show()"
      ],
      "metadata": {
        "id": "fXBKRfFc-pdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0399363e-5cbd-4d90-e85d-9448e0bb63dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
            "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
            "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
            "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
            "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
            "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
            "|   Mahesh|NULL|      NULL| 40000|         21|                10|         40000|\n",
            "|     NULL|  34|        10| 38000|         34|                10|         38000|\n",
            "|     NULL|  36|      NULL|  NULL|         36|                10|         20000|\n",
            "+---------+----+----------+------+-----------+------------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Filter Operations\n",
        "\n",
        "We will be retrieving records based on some conditions\n",
        "- &, |, ==\n",
        "- ~ (Not)"
      ],
      "metadata": {
        "id": "P0HehKg8-pbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/sample_dataset/test1.csv'\n",
        "df_pyspark = spark.read.csv(file_path, header = True, inferSchema = True)\n",
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "Vda9Wbet-pYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db792327-8bab-49c3-8f12-e53272027b2f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Salary of the people less than or equal to 20000\n",
        "df_pyspark.filter('Salary <= 20000').show()"
      ],
      "metadata": {
        "id": "6AcOHoGh-pVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e2245ae-2d8c-417e-81d9-c6e8010d81a2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Selecting only specific columns\n",
        "df_pyspark.filter('Salary <= 20000').select(['Name', 'Salary']).show()"
      ],
      "metadata": {
        "id": "036xV6UT-pS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b89e337-2f18-476f-f2e7-7ccda74169bf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   Name|Salary|\n",
            "+-------+------+\n",
            "|  Sunny| 20000|\n",
            "|   Paul| 20000|\n",
            "| Harsha| 15000|\n",
            "|Shubham| 18000|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to select specific condition\n",
        "df_pyspark.filter(df_pyspark['Salary']<= 20000).show()"
      ],
      "metadata": {
        "id": "5WxtFEyF-pP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a637e3b8-57a6-4987-8878-120d7d8d4df6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using ~ (Not): Inverse condition or Inverse filter operation\n",
        "df_pyspark.filter(~(df_pyspark['Salary']<=20000)).show()"
      ],
      "metadata": {
        "id": "8J_shjbcZxNe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9f3891-699c-4805-d7a3-337f8e659ad1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting multiple conditions. Using &\n",
        "df_pyspark.filter((df_pyspark['Salary']<= 20000) &\n",
        "                  (df_pyspark['Salary'] >= 15000)).show()"
      ],
      "metadata": {
        "id": "qg2w2589-pM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c37f877-d53b-4e4b-b5e4-3e2ecb209e2b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting multiple conditions. Using or (|)\n",
        "df_pyspark.filter((df_pyspark['Salary']<= 20000) |\n",
        "                  (df_pyspark['Salary'] >= 15000)).show()"
      ],
      "metadata": {
        "id": "dmOw4A9f-pKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4914ea25-2b66-4197-8697-a7d08ab5414e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pyspark GroupBy and Aggregate functions"
      ],
      "metadata": {
        "id": "Y-nAZg3f-pHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark = spark.read.csv('/content/drive/My Drive/sample_dataset/test3.csv',header = True, inferSchema=True)"
      ],
      "metadata": {
        "id": "5doZ4MwL-pEU"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.show()"
      ],
      "metadata": {
        "id": "Y1CLk1WF-pBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4aeaf7f-6597-43e9-fa41-ed0c50390997"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+------+\n",
            "|     Name| Departments|salary|\n",
            "+---------+------------+------+\n",
            "|    Krish|Data Science| 10000|\n",
            "|    Krish|         IOT|  5000|\n",
            "|   Mahesh|    Big Data|  4000|\n",
            "|    Krish|    Big Data|  4000|\n",
            "|   Mahesh|Data Science|  3000|\n",
            "|Sudhanshu|Data Science| 20000|\n",
            "|Sudhanshu|         IOT| 10000|\n",
            "|Sudhanshu|    Big Data|  5000|\n",
            "|    Sunny|Data Science| 10000|\n",
            "|    Sunny|    Big Data|  2000|\n",
            "+---------+------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# checking datatypes of columns\n",
        "df_pyspark.printSchema()"
      ],
      "metadata": {
        "id": "56hrPwGD-o-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5269c8-0f88-4fb9-9e49-05ae5689afdb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Departments: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Groupby\n",
        "# Grouped to find maximum salary\n",
        "df_pyspark.groupBy('Name').sum().show()"
      ],
      "metadata": {
        "id": "oQNkGNu7-o7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1b19e8-5c75-4d69-ebfc-068dbbfd6fcb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|     Name|sum(salary)|\n",
            "+---------+-----------+\n",
            "|Sudhanshu|      35000|\n",
            "|    Sunny|      12000|\n",
            "|    Krish|      19000|\n",
            "|   Mahesh|       7000|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Departments which gives maximum salary\n",
        "df_pyspark.groupBy('Departments').sum().show()"
      ],
      "metadata": {
        "id": "KJzNnSwm-o4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c08d6f8-d625-4062-8f6a-a375ec9d6730"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+\n",
            "| Departments|sum(salary)|\n",
            "+------------+-----------+\n",
            "|         IOT|      15000|\n",
            "|    Big Data|      15000|\n",
            "|Data Science|      43000|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Departments which gives maximum salary on average\n",
        "df_pyspark.groupBy('Departments').mean().show()"
      ],
      "metadata": {
        "id": "bUSRg4ld-o11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f12afc6-2b92-4898-8cf9-9522a5953fc3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+\n",
            "| Departments|avg(salary)|\n",
            "+------------+-----------+\n",
            "|         IOT|     7500.0|\n",
            "|    Big Data|     3750.0|\n",
            "|Data Science|    10750.0|\n",
            "+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How many people woking in the department\n",
        "df_pyspark.groupBy('Departments').count().show()"
      ],
      "metadata": {
        "id": "aTpX1V-1-oyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d36b379-21b9-433a-95dd-df0f3bce3aa0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "| Departments|count|\n",
            "+------------+-----+\n",
            "|         IOT|    2|\n",
            "|    Big Data|    4|\n",
            "|Data Science|    4|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Who's getting the max salary\n",
        "df_pyspark.groupBy('Name').max().show()"
      ],
      "metadata": {
        "id": "Mv_z1O_JO89Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d57462d-520e-40e8-90b9-2156337a7645"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|     Name|max(salary)|\n",
            "+---------+-----------+\n",
            "|Sudhanshu|      20000|\n",
            "|    Sunny|      10000|\n",
            "|    Krish|      10000|\n",
            "|   Mahesh|       4000|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_pyspark.agg({'Salary':'sum'}).show()"
      ],
      "metadata": {
        "id": "7K5DAPcy-ov2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb88fad8-d909-4d3c-9eb2-9d2b22317060"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|sum(Salary)|\n",
            "+-----------+\n",
            "|      73000|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Pyspark ML"
      ],
      "metadata": {
        "id": "PSEVxmmr-otF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "training = spark.read.csv('/content/drive/My Drive/sample_dataset/test1.csv', inferSchema = True, header = True)"
      ],
      "metadata": {
        "id": "_4PVA_sI-onb"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training.show()"
      ],
      "metadata": {
        "id": "v5rRmcA6-okv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d354cdbb-9516-40d4-f03f-f91b3e855453"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+\n",
            "|     Name|age|Experience|Salary|\n",
            "+---------+---+----------+------+\n",
            "|    Krish| 31|        10| 30000|\n",
            "|Sudhanshu| 30|         8| 25000|\n",
            "|    Sunny| 29|         4| 20000|\n",
            "|     Paul| 24|         3| 20000|\n",
            "|   Harsha| 21|         1| 15000|\n",
            "|  Shubham| 23|         2| 18000|\n",
            "+---------+---+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.printSchema()"
      ],
      "metadata": {
        "id": "IV7FypqV-ohq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a97f62-cb49-4b52-c14d-e536e40bbde9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- Experience: integer (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training.columns"
      ],
      "metadata": {
        "id": "l-aFhZ_R-oe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6611eac9-3576-4d44-eccd-66dc7def9cf6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Name', 'age', 'Experience', 'Salary']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VectorAssembler: It will make sure that I have my features grouped like this [age, Experience], and we will trat this as new feature and It will be an independant feature.\n",
        "- [age, Experience]--->new feature--->Independent feature"
      ],
      "metadata": {
        "id": "qoGPY-egYjsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "featureassembler = VectorAssembler(inputCols=['age', 'Experience'], outputCol = 'Independent Feature')"
      ],
      "metadata": {
        "id": "7eDvuRW0-ocJ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = featureassembler.transform(training)"
      ],
      "metadata": {
        "id": "9bF27XGqYiGA"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.show()"
      ],
      "metadata": {
        "id": "jpJo5Gl_-oZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e21ab85-6c76-4630-fe00-951ed45876fd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------+-------------------+\n",
            "|     Name|age|Experience|Salary|Independent Feature|\n",
            "+---------+---+----------+------+-------------------+\n",
            "|    Krish| 31|        10| 30000|        [31.0,10.0]|\n",
            "|Sudhanshu| 30|         8| 25000|         [30.0,8.0]|\n",
            "|    Sunny| 29|         4| 20000|         [29.0,4.0]|\n",
            "|     Paul| 24|         3| 20000|         [24.0,3.0]|\n",
            "|   Harsha| 21|         1| 15000|         [21.0,1.0]|\n",
            "|  Shubham| 23|         2| 18000|         [23.0,2.0]|\n",
            "+---------+---+----------+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finalized_data = output.select('Independent Feature', 'Salary')"
      ],
      "metadata": {
        "id": "7YCzJEXp-oWO"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalized_data.show()"
      ],
      "metadata": {
        "id": "CsUOYizk-oTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9246380-8a07-40d4-ede4-94668558da2d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------+\n",
            "|Independent Feature|Salary|\n",
            "+-------------------+------+\n",
            "|        [31.0,10.0]| 30000|\n",
            "|         [30.0,8.0]| 25000|\n",
            "|         [29.0,4.0]| 20000|\n",
            "|         [24.0,3.0]| 20000|\n",
            "|         [21.0,1.0]| 15000|\n",
            "|         [23.0,2.0]| 18000|\n",
            "+-------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "## train test split\n",
        "train_df, test_df = finalized_data.randomSplit([0.75,0.25])\n",
        "regressor = LinearRegression(featuresCol = 'Independent Feature',labelCol='Salary')\n",
        "regressor = regressor.fit(train_df)"
      ],
      "metadata": {
        "id": "gUSCAiU7-oQu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.coefficients"
      ],
      "metadata": {
        "id": "qH6_rglt-oOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa57140-04b6-457e-c5e4-7fb2df8573a1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([-323.2867, 1696.8066])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regressor.intercept"
      ],
      "metadata": {
        "id": "86Vn5J0Z-oLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27ffd596-bc4b-4abe-db75-883f04aae806"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22295.299605311826"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = regressor.evaluate(test_df)"
      ],
      "metadata": {
        "id": "YhHHRreo-oIv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred.predictions.show()"
      ],
      "metadata": {
        "id": "iEW9QaWK-oF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ef39f5-1ea1-42c5-8995-1a9ba9957028"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+------+------------------+\n",
            "|Independent Feature|Salary|        prediction|\n",
            "+-------------------+------+------------------+\n",
            "|         [21.0,1.0]| 15000|17203.085755292585|\n",
            "+-------------------+------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.meanSquaredError, pred.meanAbsoluteError"
      ],
      "metadata": {
        "id": "GhnTfST5-oDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee7b253-4f25-47aa-b76d-59f4443d6a23"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4853586.845173097, 2203.0857552925845)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vt4mrlxG-oAj"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JKMpl-cV-n98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hocD9lb-n65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hplpplRr-n4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1_laF-p-n1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_j_j-cp6-nzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_Q0eXyx-nwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOS86-oF-nuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqOwGGQH-nrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7XYxj6wo-no0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZMvKkfuI-nl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lxk_K-m6-njo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}